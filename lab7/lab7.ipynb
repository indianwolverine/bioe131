{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100% zeros\n",
    "zero_100 = np.random.choice([0, 1], size=(8 * 1024 * 1024 * 100), replace=True, p=[1.0, 0.0]) \n",
    "zero_100 = np.packbits(zero_100)\n",
    "open(\"zero_100\", \"wb\").write(zero_100)\n",
    "\n",
    "# 90% zeros\n",
    "zero_90 = np.random.choice([0, 1], size=(8 * 1024 * 1024 * 100), replace=True, p=[0.9, 0.1]) \n",
    "zero_90 = np.packbits(zero_90)\n",
    "open(\"zero_90\", \"wb\").write(zero_90)\n",
    "\n",
    "# 80% zeros\n",
    "zero_80 = np.random.choice([0, 1], size=(8 * 1024 * 1024 * 100), replace=True, p=[0.8, 0.2]) \n",
    "zero_80 = np.packbits(zero_80)\n",
    "open(\"zero_80\", \"wb\").write(zero_80)\n",
    "\n",
    "# 70% zeros\n",
    "zero_70 = np.random.choice([0, 1], size=(8 * 1024 * 1024 * 100), replace=True, p=[0.7, 0.3]) \n",
    "zero_70 = np.packbits(zero_70)\n",
    "open(\"zero_70\", \"wb\").write(zero_70)\n",
    "\n",
    "# 60% zeros\n",
    "zero_60 = np.random.choice([0, 1], size=(8 * 1024 * 1024 * 100), replace=True, p=[0.6, 0.4]) \n",
    "zero_60 = np.packbits(zero_60)\n",
    "open(\"zero_60\", \"wb\").write(zero_60)\n",
    "\n",
    "# 50% zeros\n",
    "zero_50 = np.random.choice([0, 1], size=(8 * 1024 * 1024 * 100), replace=True, p=[0.5, 0.5]) \n",
    "zero_50 = np.packbits(zero_50)\n",
    "open(\"zero_50\", \"wb\").write(zero_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uniformly random DNA string\n",
    "\n",
    "dna = np.random.choice([\"A\", \"T\", \"G\", \"C\"], size=(100000000), replace=True, p=[0.25, 0.25, 0.25, 0.25])\n",
    "open(\"dna\", \"w\").write(\"\".join(dna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uniformly random Protein string\n",
    "\n",
    "protein_choice = [\"G\", \"A\", \"L\", \"M\", \"F\", \"W\", \"K\", \"Q\", \"E\", \"S\", \"P\", \"V\", \"I\", \"C\", \"Y\", \"H\", \"R\", \"N\", \"D\", \"T\"]\n",
    "probs = [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
    "protein = np.random.choice(protein_choice, size=(100000000), replace=True, p=probs)\n",
    "open(\"protein\", \"w\").write(\"\".join(protein))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compression procedure in terminal\n",
    "- time gzip -c zero_100 > zero_100.gz\n",
    "- time bzip2 -k zero_100 \n",
    "- time pbzip2 -fk zero_100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Filename      | Size Input | Size Output (gzip) | Size Output (bzip2) | Size Output (pbzip2) | time (gzip) | time (bzip2) | time (pbzip2) \n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| zero_100    | 100M        | 100K        |  113B       | 5.5K        | 0.617s      | 0.923s      | 0.145s      |\n",
    "| zero_90     | 100M        | 57M         |  59M        | 59M         | 27.670s     | 10.208s     | 1.129s      |\n",
    "| zero_80     | 100M        | 78M         |  83M        | 83M         | 18.227s     | 10.769s     | 1.337s      |\n",
    "| zero_70     | 100M        | 90M         |  96M        | 96M         | 7.293s      | 11.574s     | 1.575s      |\n",
    "| zero_60     | 100M        | 98M         |  101M       | 101M        | 5.699s      | 12.818s     | 1.855s      |\n",
    "| zero_50     | 100M        | 101M        |  101M       | 101M        | 3.654s      | 13.532s     | 2.025s      |\n",
    "| dna         | 96M         | 28M         |  27M        | 27M         | 23.588s     | 9.495s      | 1.026s      |\n",
    "| protein     | 96M         | 58M         |  53M        | 53M         | 4.340s      | 9.603s      | 1.047s      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm achieves the best level of compression on each file type?\n",
    "- On the zero/one files gzip performs marginally better than bzip2/pbzip2, and on the dna and protein sequences, \n",
    "    bzip2/pbzip2 perform marginally better than gzip.\n",
    "\n",
    "Which algorithm is the fastest?\n",
    "- pbzip2\n",
    "\n",
    "What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why? \n",
    "- pbzip2 is simply the parellelized version of bzip2, which means it does the same work as bzip2 but is run concurrently \n",
    "    on a computer, thus allowing it to be faster.\n",
    "\n",
    "How does the level of compression change as the percentage of zeros increases? Why does this happen?\n",
    "- As the percentage of zeros increases to 50%, the entropy of the data increases, making it harder for the compression algorithms to find patterns in the data and compress it. Thus we see a decline in compression ratio.\n",
    "\n",
    "What is the minimum number of bits required to store a single DNA base? \n",
    "- 2\n",
    "\n",
    "What is the minimum number of bits required to store an amino acid letter? \n",
    "- 5\n",
    "\n",
    "In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and \n",
    "protein sequences? \n",
    "- gzip took 2.24 x 10^8 bits to store the dna sequence, while bzip2 required 2.16 x 10^8 bits to store the same sequence. As for protein, gzip took 4.64 x 10^8 bits and bzip2 took 4.24 x 10^8 bits.\n",
    "\n",
    "Are gzip and bzip2 performing well on DNA and proteins? \n",
    "- For dna sequences, the ideal number of bits to encode a sequence should be 2 * sequence_length, since from an information theory perspective, if the bases are uniformly randomly distributed, it should take 2 bits to encode each base. gzip and bzip2 both perform worse than this ideal, and we should simply encode these dna sequences in 2bit format instead. For protein sequences, however, since we can't have fractional bits, it should take 5 bits to encode each amino acid uniquely, giving us a file length of 5 * amino_acid_sequence_length. In this regard, both gzip and bzip2 compress the protein sequence to less than 5 * amino_acid_sequence_length, showing that they perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing Real Data\n",
    "\n",
    "A priori, do you expect to achieve better or worse compression here than random data? Why? \n",
    "- Better compression since nucleotides in real sequences are not uniformly randomly distributed and thus the compression algorithms should be able to exploit patterns in the data\n",
    "\n",
    "How does the compression ratio of this file compare to random data? \n",
    "- gzip compression ratio = 2.1KB / 795B = 2.64\n",
    "- bzip2 compression ratio = 2.1KB / 878B = 2.39\n",
    "\n",
    "These ratios are less than the compression ratio acheived in the random data (3.42 and 3.55 for gzip and bzip2 respectively), likely because we provided too few sequences for the compression algorithms to be able to exploit patterns in the data. With a larger dataset, we likely would have seen better compression ratios on the real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating compression of 1000 Terabytes\n",
    "\n",
    "Given the benchmarking data you obtained in this lab, which algorithm do you propose to use for each type of data? Provide an estimate for the fraction of space you can save using your compression scheme. How much of a bonus do you anticipate receiving this year?\n",
    "- Seeing as bzip2 performed best on dna-like data, I would choose pbzip2 to compress the genomic and plasmid data, with pbzip2 working fast due to the parellelism. For the protein data I would also choose pbzip2 due to the speed and superior compression ratios. For the binary microscope data I would choose gzip because it performed better on more random data than bzip2.\n",
    "- Given the gathered data, I would expect to acheive a compression ratio of around 4 for the genomic and protein data, and 1 for the random binary data. With 90% of the data compressed by 4 times, I would save 225 Terabytes of data, but in fact I would only be able to compress an even smaller percentage of the data. Given that I can compress about 100 MB of data in about 1 second using pbzip2, in 24 hours I could compress 8.64 TB of data. In this best case, we would cut down 8.64 TB of data to 2.16 TB, saving 6.48 TB. Given a cost savings of 50 dollars per TB of disk space saved, we would save 324 dollars per day. In a year then, we would save 118,260 dollars! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
